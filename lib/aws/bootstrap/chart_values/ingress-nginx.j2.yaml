controller:
  useComponentLabel: true
  admissionWebhooks:
    enabled: true # set-by-engine-code
  allowSnippetAnnotations: true
  # enable if you want metrics scrapped by prometheus
  metrics:
    enabled: true # set-by-engine-code
    serviceMonitor:
      enabled: false # set-by-engine-code
  config:
    # set global default file size limit to 100m
    proxy-body-size: 100m
    # hide Nginx version
    server-tokens: "false"
    # annotation risk level - needed because of nginx.ingress.kubernetes.io/configuration-snippet in ingress configuration
    annotations-risk-level: "Critical"
    # required for X-Forwarded-for to work with ALB controller
    use-proxy-protocol: "false"
    # enable real IP (client IP)
    enable-real-ip: "false" # set-by-engine-code
    # passes the incoming X-Forwarded-* headers to upstreams
    use-forwarded-headers: "false" # set-by-engine-code
    # append the remote address to the X-Forwarded-For header instead of replacing it
    compute-full-forwarded-for: "false" # set-by-engine-code
    # customize http-snippet
    {%- if nginx_controller_http_snippet %}
    http-snippet: |
        {{ nginx_controller_http_snippet | indent(prefix="        ") }}
    {%- endif %}
    {%- if nginx_controller_server_snippet %}
    server-snippet: |
      {{ nginx_controller_server_snippet | indent(prefix="        ") }}
    {%- endif %}
    # customize log-format / set-by-engine-code
    # default format can be found in the template: https://github.com/nginxinc/kubernetes-ingress/blob/v3.5.2/internal/configs/version1/nginx.tmpl#L44
    # nginx_controller_log_format_upstream can be a json that why we pass it in the value file
    {%- if nginx_controller_log_format_upstream %}
    log-format-upstream: >
      {{ nginx_controller_log_format_upstream }}
    {%- endif %}
    log-format-escaping-json: "false" # set-by-engine-code
    log-format-escaping-none: "false" # set-by-engine-code
    limit-req-status-code: "503" # set-by-engine-code
    # Compression configuration (enabled by default)
    {%- if nginx_controller_enable_compression is not defined or nginx_controller_enable_compression %}
    enable-brotli: "true"
    brotli-level: "6"
    brotli-types: "text/xml text/yaml image/svg+xml application/x-font-ttf image/vnd.microsoft.icon application/x-font-opentype application/json font/eot application/vnd.ms-fontobject application/javascript font/otf application/xml application/xhtml+xml text/javascript application/x-javascript text/plain application/x-font-truetype application/xml+rss image/x-icon font/opentype text/css image/x-win-bitmap"
    use-gzip: "true"
    gzip-level: "6"
    gzip-types: "text/xml text/yaml image/svg+xml application/x-font-ttf image/vnd.microsoft.icon application/x-font-opentype application/json font/eot application/vnd.ms-fontobject application/javascript font/otf application/xml application/xhtml+xml text/javascript application/x-javascript text/plain application/x-font-truetype application/xml+rss image/x-icon font/opentype text/css image/x-win-bitmap"
    {%- endif %}

  # PDB
  maxUnavailable: 20%

  ingressClassResource:
    # -- Name of the IngressClass
    name: nginx-qovery
    # -- Create the IngressClass or not
    enabled: true

  # the Ingress Class name to be used by Ingresses (use "nginx-qovery" for Qovery application/container deployments)
  ingressClass: nginx-qovery

  extraArgs:
    # Kubernetes path of the default Cert-manager TLS certificate (if used)
    default-ssl-certificate: "cert-manager/letsencrypt-acme-qovery-cert"
  updateStrategy:
    rollingUpdate:
      # AWS LB is slow to catchup change in the topology, so we go 1 by 1 to not have any downtime
      maxSurge: 1
      maxUnavailable: 0

  # AWS LB is slow to catchup change in the topology, so we go slowly to let AWS catchup change before moving to the next instance
  # LB healthcheck is 6, and need 2 rounds to consider the instance as (un)healthy. Double the time to be safe
  readinessProbe:
    initialDelaySeconds: 30

  # enable autoscaling if you want to scale the number of replicas based on CPU usage
  autoscaling:
    enabled: false # set-by-engine-code
    minReplicas: 2 # set-by-engine-code
    maxReplicas: 25 # set-by-engine-code
    targetCPUUtilizationPercentage: 50 # set-by-engine-code

  # required if you rely on a load balancer
  # the controller mirrors the address of this service's endpoints to the load-balancer status of all Ingress objects it satisfies.
  publishService:
    enabled: true

  # set a load balancer if you want your Nginx to be publicly accessible
  service:
    enabled: true
    annotations:
      # Qovery is in a migration process to ALB controller, we advice to use it
      service.beta.kubernetes.io/aws-load-balancer-type: nlb
      # service.beta.kubernetes.io/aws-load-balancer-type: "external"
      # service.beta.kubernetes.io/aws-load-balancer-scheme": "internet-facing"
      # service.beta.kubernetes.io/aws-load-balancer-name: "qovery-<short-cluster-id>-nginx-ingress",
      # service.beta.kubernetes.io/aws-load-balancer-proxy-protocol": "*",
      # service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: "ip",
      # service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true",
      # service.beta.kubernetes.io/aws-load-balancer-target-group-attributes: "target_health_state.unhealthy.connection_termination.enabled=false,target_health_state.unhealthy.draining_interval_seconds=300",
      # service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval: "10",
      # Qovery managed DNS requieres *.$domain (something like: *.<cluster_id>.<given_dns_name>)
      external-dns.alpha.kubernetes.io/hostname: "set-by-engine-code"
    externalTrafficPolicy: "Local"
    sessionAffinity: ""
    healthCheckNodePort: 0

  # force a connection for 30 seconds before shutting down, to avoid exiting too early
  # and let time to AWS LB to catchup change in the topology
  # When /wait-shutdown is called, the LB healthcheck /healthz endpoint return an error, but nginx keep processing request
  lifecycle:
    preStop:
      exec:
        command:
          - sh
          - -c
          - (sleep 30 | nc localhost 80)&  sleep 1 ; /wait-shutdown

  topologySpreadConstraints:
    - labelSelector:
        matchLabels:
          app.kubernetes.io/instance: nginx-ingress
          app.kubernetes.io/component: controller
      topologyKey: kubernetes.io/hostname
      maxSkew: 1
      whenUnsatisfiable: DoNotSchedule

  {%- if enable_karpenter %}
  tolerations:
    - effect: NoSchedule
      key: nodepool/stable
      operator: Exists

  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
            - key: karpenter.sh/nodepool
              operator: In
              values:
                - stable
  {%- endif %}
