controller:
  useComponentLabel: true
  admissionWebhooks:
    enabled: false
  allowSnippetAnnotations: true
  # enable if you want metrics scrapped by prometheus
  metrics:
    enabled: true # set-by-engine-code
    serviceMonitor:
      enabled: false # set-by-engine-code
  config:
    # set global default file size limit to 100m
    proxy-body-size: 100m
    # hide Nginx version
    server-tokens: "false"
    # required for X-Forwarded-for to work with ALB controller
    use-proxy-protocol: "false"
    # enable real IP (client IP)
    enable-real-ip: "false" # set-by-engine-code
    # customize log-format / set-by-engine-code
    # default format can be found in the template: https://github.com/nginxinc/kubernetes-ingress/blob/v3.5.2/internal/configs/version1/nginx.tmpl#L44
    # nginx_controller_log_format_upstream can be a json that why we pass it in the value file
    {%- if nginx_controller_log_format_upstream %}
    log-format-upstream: >
      {{ nginx_controller_log_format_upstream }}
    {%- endif %}
    log-format-escaping-json: "false" # set-by-engine-code
    log-format-escaping-none: "false" # set-by-engine-code

  # the Ingress Class name to be used by Ingresses (use "nginx-qovery" for Qovery application/container deployments)
  ingressClass: nginx-qovery
  extraArgs:
    # Kubernetes path of the default Cert-manager TLS certificate (if used)
    default-ssl-certificate: "cert-manager/letsencrypt-acme-qovery-cert"
  updateStrategy:
    rollingUpdate:
      # AWS LB is slow to catchup change in the topology, so we go 1 by 1 to not have any downtime
      maxSurge: 1
      maxUnavailable: 0

  # AWS LB is slow to catchup change in the topology, so we go slowly to let AWS catchup change before moving to the next instance
  # LB healthcheck is 6, and need 2 rounds to consider the instance as (un)healthy. Double the time to be safe
  readinessProbe:
    initialDelaySeconds: 30

  # enable autoscaling if you want to scale the number of replicas based on CPU usage
  autoscaling:
    enabled: false # set-by-engine-code
    minReplicas: 2 # set-by-engine-code
    maxReplicas: 25 # set-by-engine-code
    targetCPUUtilizationPercentage: 50 # set-by-engine-code

  # required if you rely on a load balancer
  # the controller mirrors the address of this service's endpoints to the load-balancer status of all Ingress objects it satisfies.
  publishService:
    enabled: true

  # set a load balancer if you want your Nginx to be publicly accessible
  service:
    enabled: true
    annotations:
      service.beta.kubernetes.io/aws-load-balancer-type: nlb
      # Qovery managed DNS requieres *.$domain (something like: *.<cluster_id>.<given_dns_name>)
      external-dns.alpha.kubernetes.io/hostname: "set-by-engine-code"
    externalTrafficPolicy: "Local"
    sessionAffinity: ""
    healthCheckNodePort: 0

  # force a connection for 30 seconds before shutting down, to avoid exiting too early
  # and let time to AWS LB to catchup change in the topology
  # When /wait-shutdown is called, the LB healthcheck /healthz endpoint return an error, but nginx keep processing request
  lifecycle:
    preStop:
      exec:
        command:
          - sh
          - -c
          - (sleep 30 | nc localhost 80)&  sleep 1 ; /wait-shutdown

  topologySpreadConstraints:
    - labelSelector:
        matchLabels:
          app.kubernetes.io/instance: nginx-ingress
          app.kubernetes.io/component: controller
      topologyKey: kubernetes.io/hostname
      maxSkew: 1
      whenUnsatisfiable: DoNotSchedule

  {%- if enable_karpenter %}
  tolerations:
    - effect: NoSchedule
      key: nodepool/stable
      operator: Exists

  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
            - key: karpenter.sh/nodepool
              operator: In
              values:
                - stable
  {%- endif %}