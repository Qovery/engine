controller:
  admissionWebhooks:
    enabled: false
  useComponentLabel: true
  allowSnippetAnnotations: true
  # enable if you want metrics scrapped by prometheus
  metrics:
    enabled: true # set-by-engine-code
    serviceMonitor:
      enabled: false # set-by-engine-code
  config:
    # set global default file size limit to 100m
    proxy-body-size: 100m
    # hide Nginx version
    server-tokens: "false"
    # required for X-Forwarded-for to work
    use-proxy-protocol: "true"
    # enable real IP (client IP)
    enable-real-ip: "false" # set-by-engine-code
    # passes the incoming X-Forwarded-* headers to upstreams
    use-forwarded-headers: "false" # set-by-engine-code
    # append the remote address to the X-Forwarded-For header instead of replacing it
    compute-full-forwarded-for: "false" # set-by-engine-code
    # customize log-format / set-by-engine-code
    # default format can be found in the template: https://github.com/nginxinc/kubernetes-ingress/blob/v3.5.2/internal/configs/version1/nginx.tmpl#L44
    # nginx_controller_log_format_upstream can be a json that why we pass it in the value file
    {%- if nginx_controller_log_format_upstream %}
    log-format-upstream: >
      {{ nginx_controller_log_format_upstream }}
    {%- endif %}
    log-format-escaping-json: "false" # set-by-engine-code
    log-format-escaping-none: "false" # set-by-engine-code

  ingressClassResource:
    # -- Name of the IngressClass
    name: nginx-qovery
    # -- Create the IngressClass or not
    enabled: true

  # the Ingress Class name to be used by Ingresses (use "nginx-qovery" for Qovery application/container deployments)
  ingressClass: nginx-qovery

  extraArgs:
    # Kubernetes path of the default Cert-manager TLS certificate (if used)
    default-ssl-certificate: "cert-manager/letsencrypt-acme-qovery-cert"
  updateStrategy:
    rollingUpdate:
      # set the minimum acceptable number of unavailable pods during a rolling update
      maxSurge: 1
      maxUnavailable: 0

  readinessProbe:
    initialDelaySeconds: 15

  # enable auoscaling if you want to scale the number of replicas based on CPU usage
  autoscaling:
    enabled: false # set-by-engine-code
    minReplicas: 2 # set-by-engine-code
    maxReplicas: 25 # set-by-engine-code
    targetCPUUtilizationPercentage: 50 # set-by-engine-code

  # required if you rely on a load balancer
  # the controller mirrors the address of this service's endpoints to the load-balancer status of all Ingress objects it satisfies.
  publishService:
    enabled: true

  # force a connection for 30 seconds before shutting down, to avoid exiting too early
  # and let time to AWS LB to catchup change in the topology
  # When /wait-shutdown is called, the LB healthcheck /healthz endpoint return an error, but nginx keep processing request
  lifecycle:
    preStop:
      exec:
        command:
          - sh
          - -c
          - (sleep 30 | nc localhost 80)&  sleep 1 ; /wait-shutdown

  # set a load balancer if you want your Nginx to be publicly accessible
  service:
    enabled: true
    # https://github.com/scaleway/scaleway-cloud-controller-manager/blob/master/docs/loadbalancer-annotations.md
    annotations:
      service.beta.kubernetes.io/scw-loadbalancer-forward-port-algorithm: "leastconn"
      service.beta.kubernetes.io/scw-loadbalancer-protocol-http: "false"
      service.beta.kubernetes.io/scw-loadbalancer-proxy-protocol-v1: "false"
      service.beta.kubernetes.io/scw-loadbalancer-proxy-protocol-v2: "true"
      # We use http healthcheck for the load balancer to speed up the detection of the instance being down
      # during termination of the pods.
      # With TCP healthcheck, the LB is going to keep the instance in the pool as long as the port is open,
      # even if nginx will refuse new connections, because it is shutting down.
      # Doing real http healthcheck, will minimize the time the LB will keep the instance in the pool.
      service.beta.kubernetes.io/scw-loadbalancer-health-check-type: "80:http;443:https"
      service.beta.kubernetes.io/scw-loadbalancer-health-check-http-uri: "80:/healthz;443:/healthz"
      service.beta.kubernetes.io/scw-loadbalancer-health-check-send-proxy: "true"
      service.beta.kubernetes.io/scw-loadbalancer-use-hostname: "true"
      service.beta.kubernetes.io/scw-loadbalancer-health-check-delay: "2s"
      service.beta.kubernetes.io/scw-loadbalancer-health-check-timeout: "2s"
      service.beta.kubernetes.io/scw-loadbalancer-health-check-max-retries: "2"
      service.beta.kubernetes.io/scw-loadbalancer-redispatch-attempt-count: "1"
      service.beta.kubernetes.io/scw-loadbalancer-timeout-server: "30s"
      # set Scaleway load balancer type https://www.scaleway.com/en/load-balancer/ (ex: LB-S, LB-GP-M, LB-GP-L, LB-GP-XL)
      service.beta.kubernetes.io/scw-loadbalancer-type: "LB-S" # set-by-engine-code
      # Qovery managed DNS requieres *.$domain (something like: *.<cluster_id>.<given_dns_name>)
      external-dns.alpha.kubernetes.io/hostname: "set-by-engine-code"
    externalTrafficPolicy: "Local"

  topologySpreadConstraints:
    - labelSelector:
        matchLabels:
          app.kubernetes.io/instance: nginx-ingress
          app.kubernetes.io/component: controller
      topologyKey: kubernetes.io/hostname
      maxSkew: 1
      whenUnsatisfiable: DoNotSchedule
